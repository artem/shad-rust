В данной задаче вам предложено написать простой web crawler.

## Описание

Web crawler - это приложение, которое обходит заданный web-сайт, посещая все его страницы
(те, ссылки на которые он смог достать). Алгоритм работы crawler'а следующий:

1. Достать очередной url из очереди;
2. Если этот url ещё не посещён - посетить его;
3. Достать из тела ответа новые url'ы и добавить их в очередь.

У crawler'а также есть опциональный параметр `concurrent_requests` - он определяет, какое максимальное количество запросов
crawler может задавать единовременно.

## Реализация

* Для выполнения http-запросов используйте библиотеку reqwest, работающую поверх tokio.
Базовое использование следующее:

```rust
reqwest::get(url).await.unwrap().text().await.unwrap();
```

* Чтобы найти все ссылки в теле странцы, используйте библиотеку linkify. Базовое использование:

```rust
let mut finder = LinkFinder::new();
finder.kinds(&[LinkKind::Url]);
let links = finder.links(body).map(|l| l.as_str().to_string()).collect();
```
